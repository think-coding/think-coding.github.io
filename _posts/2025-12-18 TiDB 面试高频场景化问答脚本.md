# TiDB 面试高频场景化问答脚本（运维+架构岗专属）

# 一、运维岗专属场景（侧重集群管理与问题处理）

## 场景1：如何用 TiUP 部署并扩容 TiDB 集群？

**面试官提问**：请详细说说用 TiUP 部署 3 节点 TiDB 集群（3PD+3TiKV+2TiDB）的步骤，以及后续如何扩容 1 个 TiKV 节点。

**标准回答**：部署分 5 步，前提是所有节点已配置免密登录和依赖环境：

1. 安装 TiUP：`curl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh`，并初始化环境变量；

2. 生成拓扑文件：创建 `topology.yaml`，定义各节点 IP、角色（PD/TiKV/TiDB）、端口和数据目录，比如 PD 节点配置 `host: 192.168.1.101, role: pd`；

3. 检查预部署环境：`tiup cluster check ./topology.yaml --apply`，自动修复系统参数（如关闭 swap）；

4. 部署集群：`tiup cluster deploy tidb-cluster v7.5.0 ./topology.yaml --user root`，指定集群名、版本和执行用户；

5. 启动集群：`tiup cluster start tidb-cluster`，启动后用 `tiup cluster display tidb-cluster` 确认所有节点状态为 Up。

扩容 1 个 TiKV 节点步骤：① 新建 `scale-out.yaml`，只写新增 TiKV 节点的拓扑信息；② 执行 `tiup cluster scale-out tidb-cluster ./scale-out.yaml`；③ 扩容完成后，用 `tiup cluster monitor tidb-cluster` 查看 PD 调度状态，约 10 分钟后 Region 会自动均衡到新节点。

**加分拓展**：部署时会通过 `--enable-monitor` 参数一并部署 Prometheus + Grafana 监控，默认 Grafana 端口 3000，方便后续查看集群指标。同时会在拓扑文件中配置节点 Label（如机房、可用区），为后续多 AZ 调度做准备。

## 场景2：TiKV 节点宕机，如何处理？

**面试官提问**：生产环境中 1 个 TiKV 节点突然宕机，你会怎么判断故障影响？如何恢复？如果节点彻底损坏，该怎么处理？

**标准回答**：分“故障判断-临时恢复-彻底损坏处理”三步：

1. 故障判断：① 用`tiup cluster display tidb-cluster` 确认节点状态为 Down；② 登录 PD Dashboard，查看“Region 健康度”，确认该节点上的 Region 是否已触发 Leader 重选（正常 10 秒内完成）；③ 查看业务监控，确认是否有查询报错或延迟飙升（3 副本集群下，单节点宕机不影响服务）；④ 结合服务器监控，判断是进程挂掉、硬件故障还是网络问题。

2. 临时恢复：① 若为进程挂掉，执行 `tiup cluster restart tidb-cluster -N 192.168.1.104:20160` 重启节点，重启后通过 `tikv-ctl` 工具检查数据完整性；② 若为网络波动，修复网络后等待节点自动重连，PD 会触发 Region 副本同步；③ 若为磁盘临时故障（如 IO 超时），先卸载磁盘检查，修复后重启节点，节点会通过 Raft 日志回放同步缺失数据。

3. 彻底损坏（如磁盘报废）：① 标记节点不可用：`tiup cluster drain tidb-cluster -N 192.168.1.104:20160`，让 PD 先迁移该节点上的 Leader 和 Region；② 移除故障节点：`tiup cluster scale-in tidb-cluster -N 192.168.1.104:20160`；③ 更换新节点后，按扩容流程新增 TiKV 节点，确保新节点硬件配置与现有节点一致；④ 通过 PD Dashboard 确认所有 Region 副本数恢复为 3 个，Leader 分布均衡。

**加分拓展**：我们配置了分级告警，节点宕机 1 分钟触发短信告警，5 分钟未恢复升级为电话告警。同时利用 TiDB 备份工具 BR 定期备份数据，避免极端情况下数据丢失。

## 场景3：TiDB 集群如何做备份恢复？全量+增量备份方案是什么？

**面试官提问**：我们的 TiDB 集群存储着核心交易数据，要求能实现任意时间点恢复，你会设计怎样的备份方案？如果集群故障，恢复流程是什么？

**标准回答**：基于“全量备份+增量日志”构建备份体系，满足 RPO=1 分钟、RTO=30 分钟的目标，具体方案：

1. 全量备份：用 TiDB 官方工具 BR（Backup & Restore）执行分布式快照备份，① 备份策略：每日凌晨 2 点执行全量备份，备份数据存储到对象存储（如阿里云 OSS），并设置 30 天的生命周期；② 备份命令：`tiup br backup full --pd "192.168.1.101:2379" --storage "oss://tidb-backup/20241218/" --ratelimit 100 --checksum`，通过 `--ratelimit` 限制备份速度，避免影响业务。

2. 增量备份：基于 TiDB CDC 捕获增量数据，① 部署 CDC 集群，订阅所有核心表的变更日志，同步到 Kafka 集群，Kafka 消息保留 7 天；② 用 Flink 将 Kafka 中的增量日志写入 HDFS，按小时分区存储，长期归档；③ 结合全量备份时间点和 CDC 日志，实现任意时间点恢复（PITR）。

3. 恢复流程：① 若为单表数据误删，用 `tiup br restore table` 从全量备份中恢复单表，再通过 CDC 增量日志补全到故障前状态；② 若为集群级故障，先在备用集群执行 `tiup br restore full` 恢复最新全量备份，再通过 CDC 回放增量日志，完成后切换业务流量。

**加分拓展**：我们会定期（每周）做恢复演练，在测试环境验证备份数据的可用性。同时对备份数据做加密存储，备份完成后发送校验报告到运维群，确保备份有效。

**场景4：如何优化 TiDB 集群的热点问题？**

**面试官提问**：我们的 TiDB 集群在促销活动时，经常出现部分 TiKV 节点 CPU 和 IO 飙升，其他节点却很空闲，这种热点问题该怎么解决？

**标准回答**：热点问题核心是“数据分布不均导致请求集中”，需从“发现-定位-解决-预防”四步处理，结合促销场景的优化方案：

1. 热点发现与定位：① 实时监控：通过 TiDB Dashboard 的“热点分析”页面，按“表-Region-节点”维度定位热点来源，明确是写入热点还是读取热点；② 历史分析：用 Prometheus 查询 `tikv_server_handle_query_duration_seconds` 指标，分析热点节点的请求类型和频率。促销场景多为写入热点（如订单创建）和读取热点（如商品库存查询）。

2. 针对性解决：① 写入热点优化：促销核心表（如订单表）用复合分片键 `(activity_id, user_id, id)`，将同一活动的请求分散到多个 Region；开启 TiKV 的“热点自动调度”功能，PD 会自动将热点 Region 拆分并迁移；② 读取热点优化：商品库存表用“读写分离”，通过 SQL Hint `/*+ READ_FROM_STORAGE(TIKV[FOLLOWER]) */` 将读请求路由到 Follower 节点；对高频查询的库存数据，在应用层加 Redis 缓存，设置 10 秒过期时间，减少数据库请求；③ 限流保护：在网关层对单个商品的查询 QPS 做限制（如每秒 500 次），避免突发流量压垮节点。

3. 长期预防：① 表结构优化：避免用自增主键做分片键，核心表优先用业务字段组合分片；② 调度策略：调整 PD 的 `region-split-size` 参数为 128MB，让热点 Region 更快拆分；③ 容量规划：促销前对核心 TiKV 节点扩容，确保 CPU、IO 资源冗余。

**加分拓展**：我们在促销前会做压力测试，模拟 3 倍日常流量，提前发现潜在热点并优化。同时开发了热点监控告警插件，当单个 Region 的 QPS 超 1000 时立即触发告警，运维人员可实时介入。

# 二、架构岗专属场景（侧重选型与架构设计）

## 场景5：TiDB 与 CockroachDB 选型，全球化部署如何决策？

**面试官提问**：我们公司要做全球化电商平台，北美、欧洲、东南亚都有机房，数据库在 TiDB 和 CockroachDB 之间选，你推荐哪个？理由是什么？

**标准回答**：优先推荐 CockroachDB 适配全球化部署，但需根据应用协议兼容性做灵活调整，核心决策逻辑分三层：

1. 核心场景匹配度：全球化电商的核心需求是“跨地域低延迟事务+多活写入+运维简化”，CockroachDB 更贴合：① 跨地域一致性：CockroachDB 用混合逻辑时钟（HLC）实现无中心授时，跨洲部署时事务延迟比 TiDB 低 30%+，避免 TiDB 依赖 PD Leader 发 TSO 导致的跨地域时间同步开销；② 多活写入：原生支持“全球多活”，不同地域可同时写入数据，自动处理冲突，而 TiDB 目前需基于 CDC 构建多活架构，开发成本高；③ 运维成本：对称架构无需单独部署管理节点，跨地域集群扩容时仅需新增节点，TiDB 则需在每个地域部署 PD 副本，架构更重。

2. 兼容性权衡：若公司应用基于 MySQL 开发，改造成本过高，退选 TiDB 并做架构优化：① 副本分布：通过 Placement Rules 配置“每个地域部署 1 个副本”，确保单地域故障时数据不丢失；② 性能优化：将 PD Leader 部署在核心地域（如北美），减少 TSO 分发延迟；用 TiDB 分布式执行框架，将跨地域查询拆分为本地子查询，降低网络传输开销。

3. 成本与风险控制：① 若选择 CockroachDB，可先用开源版搭建测试集群，验证跨地域事务性能，再根据需求采购商业版支持；② 若选择 TiDB，可分阶段落地，第一阶段实现“一主多备”异地灾备，第二阶段基于 CDC 构建双向同步，逐步实现多活。

**加分拓展**：调研过字节跳动的全球化数据库方案，他们在核心业务用 CockroachDB，边缘业务用 TiDB 对接 MySQL 生态，通过数据同步工具实现两库数据互通，这种混合架构可兼顾性能和兼容性。

## 场景6：TiDB 如何支撑 HTAP 混合负载？

**面试官提问**：我们需要一套数据库同时支撑电商订单交易（OLTP）和实时销量分析（OLAP），TiDB 怎么设计架构？如何避免分析查询影响交易性能？

**标准回答**：基于“计算隔离+存储分离”构建 TiDB HTAP 架构，核心是让 OLTP 和 OLAP 负载互不干扰，具体设计：

1. 整体架构分层：① 接入层：用云原生网关（如 Kong）做请求分发，按 SQL 类型路由——交易类请求（INSERT/UPDATE）路由到 OLTP 专用 TiDB Server 集群，分析类请求（SELECT COUNT/SUM）路由到 OLAP 专用 TiDB Server 集群；② 计算层：部署两组独立的 TiDB Server，OLTP 集群配置高并发参数（`tidb_max_connections=2000`），关闭并行查询以减少资源占用；OLAP 集群开启并行查询（`tidb_enable_parallel_execution=1`），配置更大的内存配额（`tidb_mem_quota_query=16GB`）；③ 存储层：TiKV 集群存储行数据，承接 OLTP 读写；给订单表、商品表创建 2 个 TiFlash 副本，TiFlash 按列存格式存储并实时同步 TiKV 数据，OLAP 查询直接扫描列存；④ 监控层：单独部署监控集群，分别监控 OLTP 和 OLAP 负载的延迟、吞吐量，设置独立告警阈值。

2. 关键优化手段：① 查询路由优化：用 SQL Hint `/*+ READ_FROM_STORAGE(TIFLASH) */` 强制分析查询走 TiFlash，同时在 TiDB 配置中开启“自动选择存储引擎”功能，优化器会根据查询类型自动选择 TiKV 或 TiFlash；② 数据隔离：给 TiKV 和 TiFlash 节点打不同的 Label（如 `engine=tikv`、`engine=tiflash`），通过 Placement Rules 确保两者部署在不同的服务器上，避免 IO 资源竞争；③ 分析查询管控：用 TiDB 的“资源管理”功能，给分析查询分配 40% 的集群资源，交易查询分配 60%，避免复杂分析 SQL 抢占资源。

3. 效果验证：交易查询延迟稳定在 50ms 内，分析查询（如“近 1 小时各品类销量排行”）延迟从原来的 15s 降至 300ms 内，且 OLAP 负载高峰期不影响 OLTP 交易成功率。

**加分拓展**：针对超大规模分析场景（如年销量统计），我们会将 TiFlash 数据同步到 ClickHouse 做离线分析，TiDB 仅承接实时性要求高的 OLAP 查询，进一步优化负载分布。

## 场景7：TiDB 与 MySQL 分库分表架构对比，金融核心系统如何选？

**面试官提问**：我们的银行核心账务系统，目前用 Sharding-JDBC + MySQL 架构，考虑迁移到 TiDB，你认为是否可行？核心差异和风险点是什么？

**标准回答**：可行但需分阶段迁移，TiDB 更适合金融核心系统的长期发展，但要重点解决兼容性和稳定性问题，核心对比与决策：

1. 核心差异（TiDB 优势）：① 分布式事务可靠性：TiDB 原生支持分布式事务，基于 Percolator 模型+2PC 实现，数据一致性达 99.999%，而 Sharding-JDBC 需依赖 Seata 等中间件实现分布式事务，一致性保障较弱；② 运维复杂度：TiDB 自动分片和扩容，无需人工维护分表规则，核心系统扩容时可做到业务无感知，Sharding-JDBC 新增分片需手动迁移数据，易出错；③ 高可用能力：TiDB 基于 Raft 协议实现多副本，节点故障自动恢复，RTO<30 秒，Sharding-JDBC 依赖 MySQL 主从复制，故障切换需 30 秒以上，且易出现数据不一致；④ 查询能力：TiDB 支持复杂跨表查询，核心系统的报表统计无需额外聚合数据，Sharding-JDBC 跨分片查询性能差，需依赖数仓同步数据。

2. 风险点与解决方案：① 语法兼容性：TiDB 对 MySQL 存储过程、触发器支持不完善，核心系统的账务计算逻辑需从存储过程迁移到应用层，用 Java 代码实现，迁移前用 `tidb-checker` 工具扫描不兼容 SQL；② 性能适配：金融系统对延迟敏感，需优化 TiDB 参数——调大 TiKV RocksDB 的 `write-buffer-size` 至 512MB，减少刷盘延迟；将事务隔离级别设为 RC，降低锁竞争；③ 稳定性验证：迁移前在测试环境做 6 个月的压力测试，模拟峰值 TPS 20000+ 的场景，验证集群稳定性；上线后先迁移非核心账务模块（如信用卡还款），运行 1 个月无问题再迁移核心模块（如存款交易）。

3. 迁移路径：采用“双写并行”方案，① 阶段一：应用同时写入 Sharding-JDBC 集群和 TiDB 集群，用 CDC 同步增量数据，确保两库数据一致；② 阶段二：将查询流量逐步切到 TiDB，监控延迟和成功率；③ 阶段三：停止写入 Sharding-JDBC 集群，完成迁移。

**加分拓展**：招商银行已将部分核心业务迁移到 TiDB，他们通过“应用层适配+TiDB 内核优化”的方式，将交易延迟控制在 20ms 内，满足金融级要求。我们可参考其参数配置和迁移策略。

## 场景8：TiDB 集群的同城多 AZ 与异地灾备架构如何设计？

**面试官提问**：我们的 TiDB 集群要支撑电商核心业务，要求满足“同城故障不中断，异地故障可恢复”，你会设计怎样的高可用架构？RPO 和 RTO 能达到什么水平？

**标准回答**：设计“同城三 AZ 多活 + 异地双活灾备”架构，核心是通过副本分布和数据同步实现高可用，目标 RPO=0、RTO<10 分钟，具体设计：

1. 同城多 AZ 架构（核心生产集群）：① 部署规划：在同城三个 AZ（AZ1/AZ2/AZ3）部署集群，PD 集群 3 节点分别部署在三个 AZ，每个 AZ 部署 2 个 TiDB Server、3 个 TiKV 节点、2 个 TiFlash 节点；② 副本策略：通过 Placement Rules 配置 TiKV Region 副本分布——1 个 Leader 副本 + 2 个 Follower 副本，确保每个副本落在不同 AZ，PD 自动将 Leader 均衡分布在三个 AZ；③ 高可用保障：单个 AZ 故障（如 AZ1 断电），该 AZ 内的 TiKV 节点不可用，但每个 Region 仍有 2 个副本在 AZ2/AZ3 存活，PD 会在 10 秒内触发 Leader 重选，业务请求自动路由到其他 AZ 的 TiDB Server，服务无中断；④ 网络优化：三个 AZ 间用万兆专线连接，确保 Raft 日志同步延迟<1ms。

2. 异地灾备架构（备用集群）：① 部署位置：在距离主集群 500 公里外的城市部署灾备集群，规模为主集群的 1/2；② 数据同步：用 TiDB CDC 订阅主集群的所有核心表变更，同步到灾备集群，同步延迟控制在 1 秒内；同时主集群每日全量备份数据同步到灾备集群的对象存储，作为兜底；③ 灾备切换：当主集群所在城市发生故障时，通过 TiUP 工具执行 `tiup cluster promote` 命令将灾备集群提升为主集群，更新 DNS 解析切换业务流量，RTO 可控制在 10 分钟内；④ 验证机制：每月执行一次灾备演练，模拟主集群故障，验证切换流程和数据一致性。

3. 关键技术支撑：① 自动故障检测：用 TiDB 监控插件检测 AZ 级故障，自动触发 Leader 重选和流量切换；② 数据一致性校验：定期用 `tiup cluster checksum` 工具对比主备集群的数据一致性，确保灾备数据可用；③ 跨集群备份：主备集群的备份数据分别存储在不同地域的对象存储，避免备份数据丢失。

**加分拓展**：该架构已在京东电商落地，他们通过引入云厂商的负载均衡服务，实现跨 AZ 流量的自动调度，当某个 AZ 负载过高时，自动将请求分发到其他 AZ，进一步提升集群的可用性和性能。

## 场景9：TiDB 与 Redis、ClickHouse 如何协同构建数据架构？

**面试官提问**：我们需要一套数据架构，既支撑高并发交易，又能做实时缓存和离线分析，TiDB 如何与 Redis、ClickHouse 配合？各组件的角色是什么？

**标准回答**：构建“Redis 缓存层 + TiDB 交易层 + ClickHouse 分析层”的三级架构，各组件各司其职又协同联动，具体分工与协作机制：

1. 各组件核心角色：① Redis：一级缓存层，存储高频访问的热点数据，如商品详情、用户会话、库存计数，缓存策略为“读写穿透”，更新数据时先更数据库再删缓存；② TiDB：核心交易层，存储全量业务数据，支撑订单创建、支付、用户管理等 OLTP 场景，同时通过 TiFlash 承接实时分析请求（如近 1 小时销量）；③ ClickHouse：离线分析层，存储历史数据（如 3 个月前的订单），支撑复杂报表统计（如月度销售分析、用户画像），数据延迟可接受在 1 小时内。

2. 数据流转机制：① 实时链路：用户请求先到 Redis，缓存未命中则查询 TiDB，同时将结果写入 Redis；TiDB 的数据变更通过 CDC 同步到 Kafka，Flink 消费 Kafka 数据，实时更新 Redis 中的热点数据（如库存）和 TiFlash 中的分析数据；② 离线链路：每日凌晨 3 点，用 Dumpling 工具导出 TiDB 中的历史数据（3 个月前），通过 DataX 同步到 ClickHouse，同步完成后删除 TiDB 中的历史数据，释放存储空间；③ 查询路由：应用层通过中间件判断查询类型——高频简单查询走 Redis，交易查询走 TiDB，实时分析走 TiFlash，离线报表走 ClickHouse。

3. 协同优化点：① 缓存一致性：用 Redis 的过期时间（如 5-10 秒）和 CDC 实时更新结合，避免缓存脏数据；② 数据分层：TiDB 只保留 3 个月内的热数据，提升查询性能；ClickHouse 用分区表存储历史数据，按天分区，查询时只扫描相关分区；③ 故障隔离：Redis 集群故障时，应用层自动降级到直接查询 TiDB，不影响核心交易；ClickHouse 故障时，离线分析请求暂停，不影响 OLTP 业务。

**加分拓展**：我们引入了分布式缓存中间件 Codis 管理 Redis 集群，实现缓存的水平扩展和故障自动切换；用 ClickHouse 的副本表和分布式表功能，提升分析查询的性能和可用性，整个架构可支撑每日 10 亿级交易和 TB 级数据分析。

## 场景10：TiDB 集群的硬件选型与容量规划如何做？

**面试官提问**：我们要搭建一个支撑日均 1000 万订单的 TiDB 集群，你会怎么做硬件选型和容量规划？如何避免资源浪费和性能瓶颈？

**标准回答**：基于“业务量估算-组件选型-容量冗余”的思路，结合订单系统的 IO 特征（写多读多），硬件选型和容量规划如下：

1. 业务量估算：日均 1000 万订单，单订单数据约 1KB，每日新增数据 10GB，按保留 1 年计算，总数据量约 4TB（含索引和副本）；峰值 TPS 约 2000（日常）、10000（促销），QPS 约 5000（日常）、30000（促销）。

2. 各组件硬件选型：① TiDB Server：核心是 CPU 密集型，选 2 路 16 核 Intel Xeon Gold 6330（3.0GHz），内存 64GB（避免 SQL 执行内存不足），磁盘用 1TB SATA SSD（仅存日志），部署 4 节点（日常 2 节点足够，促销时扩容至 4 节点）；② PD Server：元数据管理，选 1 路 8 核 Intel Xeon Silver 4314，内存 32GB，磁盘用 500GB NVMe SSD（低延迟存储元数据），部署 3 节点（奇数节点保证高可用）；③ TiKV Server：IO 密集型，选 2 路 24 核 Intel Xeon Gold 6330，内存 128GB（RocksDB 缓存需求），磁盘用 4 块 2TB NVMe SSD（做 RAID 10，提升 IO 性能和可靠性），部署 6 节点（3 副本，日常 4 节点，促销扩容至 6 节点）；④ TiFlash Server：分析场景，配置与 TiKV 一致，部署 2 节点（2 副本）；⑤ 网络：所有节点配备双万兆网卡，做链路聚合，避免网络瓶颈。

3. 容量规划与优化：① 存储容量：单 TiKV 节点有效存储约 3TB（4 块 2TB RAID 10 后可用 4TB，预留 25% 空间），6 节点可满足 4TB 数据需求，同时预留 50% 冗余应对业务增长；② 性能冗余：日常场景下，TiKV 节点 CPU 利用率控制在 40% 以内，IOPS 控制在 1 万以内（NVMe SSD 上限 3 万），促销时可承受 3 倍流量；③ 扩展规划：采用机架式服务器，预留机柜空间，扩容时直接新增节点，通过 TiUP 实现无感知扩容；④ 成本优化：非核心环境（测试、预发）用云服务器部署，降低硬件采购成本，生产环境用物理机保证性能。

4. 验证与调整：通过压测工具 SysBench 模拟峰值流量，验证硬件性能是否满足需求；上线后每月分析集群资源使用率，根据业务增长调整容量规划，避免资源浪费。

**加分拓展**：引入云原生理念，生产环境用物理机保障性能，同时部署一套轻量的云服务器集群作为容灾备用，平时承担部分查询流量，故障时快速接管核心业务，兼顾性能和成本。

# 一、通用基础场景（所有岗位必问）

## 场景1：请结合你的项目，说说为什么选择 TiDB 而非 MySQL 分库分表？

**面试官提问**：你之前做的电商订单系统，为什么最终用 TiDB 替代了 Sharding-JDBC + MySQL 的架构？核心考量是什么？

**标准回答**：我们的订单系统日均交易 500 万单，历史数据存 3 年（约 500GB），用 Sharding-JDBC 时遇到 3 个核心问题：① 跨分片查询复杂，比如“查用户近 3 年所有订单”需聚合 16 个分表结果，延迟超 500ms；② 扩容痛苦，每次新增分表需手动迁移历史数据，业务中断 2 小时以上；③ 分布式事务不可靠，支付回调更新订单状态时，偶发跨分片事务失败导致数据不一致。

换成 TiDB 后这些问题全解决了：① 无需手动分片，订单表按 user_id + id 复合分片，用户维度查询延迟降至 50ms 内；② 扩容时新增 TiKV 节点，PD 自动调度数据，业务无感知；③ 原生支持分布式事务，支付场景数据一致性达标 99.999%。另外 TiFlash 还能直接承接实时销量统计，不用再同步数据到 ClickHouse，架构简化了很多。

**加分拓展**：迁移时我们用 Dumpling 导出旧数据，TiDB Lightning 物理导入（1 小时完成 500GB 数据迁移），同步增量数据用 TiDB CDC 对接 Kafka，做到了 0 停机迁移。

## 场景2：TiDB 集群出现性能抖动，你会怎么排查？

**面试官提问**：线上 TiDB 集群突然出现 QPS 下降、延迟飙升，你作为负责人，排查流程是什么？举个你经历过的案例。

**标准回答**：我的排查逻辑是“从应用到数据库，从表层到内核”，分 4 步：① 先看应用层，确认是集群问题还是单接口问题，排除应用线程池满的情况；② 登录 TiDB Dashboard，查看“慢查询”和“SQL 分析”，定位是否有低效 SQL 突发增长；③ 查看 TiKV 监控，重点看 Leader 分布、磁盘 IOPS、Raft 同步延迟；④ 查看 PD 监控，确认是否有大量 Region 调度或 Leader 频繁切换。

之前遇到过一次延迟飙升，Dashboard 显示“订单表按 create_time 范围查询”的慢查询占比 80%，进一步看 TiKV 监控发现，该查询涉及的 Region Leader 全集中在 1 个 TiKV 节点，IOPS 达到 SSD 上限（3 万 IOPS）。原因是分片键用了 id 自增，create_time 索引数据分散，导致范围查询扫描多个 Region 且 Leader 集中。解决方法是给订单表加了 (user_id, create_time) 联合索引，同时调整 PD 调度策略，将 Leader 均衡到 3 个 TiKV 节点，延迟从 800ms 降至 80ms。

**加分拓展**：我们后来做了预防机制——用 Prometheus 配置告警规则，当单个 TiKV 节点 Leader 占比超 40% 或 IOPS 超 2.5 万时，立即触发短信告警，避免问题恶化。

# 二、开发岗专属场景（侧重 SQL 与应用适配）

## 场景3：TiDB 中如何设计订单表的分片键？为什么这么设计？

**面试官提问**：假设你设计一个 1TB 订单表，业务高频场景是“查用户的所有订单”“按订单 ID 更状态”，分片键该怎么选？对比自增 id 方案说明优势。

**标准回答**：我会选 `PRIMARY KEY (user_id, id)` 作为复合分片键，id 用雪花算法生成唯一值，不依赖自增。

核心原因有 3 点：① 贴合高频查询，“查用户订单”时，所有该用户的订单都落在以 user_id 为前缀的 Region 里，只需扫描 1-2 个 Region，而自增 id 方案需扫描全表相关 Region，延迟降低 70%+；② 避免写入热点，自增 id 会让新订单全写入最后一个 Region，复合键中 user_id 分布均匀，写入请求分散到多个 TiKV 节点；③ 事务优化，用户取消多个订单的事务，能落在同一 Region 内，避免分布式事务的 2PC 开销。

如果用自增 id 做分片键，虽然数据分布均匀，但用户维度查询会跨多个 Region，且高频查询的 user_id 索引是全局索引，查询时需要二次定位，性能很差。

**加分拓展**：针对平台大商户（单用户订单超 100 万），我们会在应用层做拆分，给大商户的 user_id 加时间后缀（如 10001_2024），避免单个 user_id 对应的 Region 过大形成热点。

## 场景4：TiDB 与 MySQL 语法兼容，开发时需要注意什么？

**面试官提问**：我们的应用从 MySQL 迁移到 TiDB，开发层面需要修改哪些代码？举 3 个你踩过的坑。

**标准回答**：核心注意点是“兼容大部分语法，但避开 MySQL 特有功能”，我踩过的 3 个坑：

1. 自增主键问题：MySQL 自增主键是单机唯一，TiDB 是全局唯一，但批量插入时会出现“主键间隙”（比如一次插 10 条，id 跳 20），之前代码里依赖 id 连续生成统计订单数，导致计算错误，后来改用按 create_time 统计解决；

2. 存储过程与触发器：TiDB 对复杂存储过程支持不完善，我们之前有个“订单创建后自动生成物流单”的触发器，迁移后执行报错，改成应用层在订单插入后调用物流接口，逻辑更清晰也更可控；

3. 索引限制：MySQL 支持全文索引，我们商品表用了 FULLTEXT 索引做搜索，TiDB 不支持，后来换成 Elasticsearch 对接 TiDB CDC 同步数据，实现全文搜索，性能比 MySQL 更好。

**加分拓展**：迁移前我们用 TiDB 提供的 `tidb-checker` 工具扫描了所有 SQL 脚本，提前发现了 12 处不兼容语法，比如 `SELECT ... FOR UPDATE SKIP LOCKED` 这种 MySQL 8.0 特性，TiDB 暂不支持，提前做了代码适配。

# 三、运维岗专属场景（侧重集群管理与问题处理）

## 场景5：如何用 TiUP 部署并扩容 TiDB 集群？

**面试官提问**：请详细说说用 TiUP 部署 3 节点 TiDB 集群（3PD+3TiKV+2TiDB）的步骤，以及后续如何扩容 1 个 TiKV 节点。

**标准回答**：部署分 5 步，前提是所有节点已配置免密登录和依赖环境：

1. 安装 TiUP：`curl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh`，并初始化环境变量；

2. 生成拓扑文件：创建 `topology.yaml`，定义各节点 IP、角色（PD/TiKV/TiDB）、端口和数据目录，比如 PD 节点配置 `host: 192.168.1.101, role: pd`；

3. 检查预部署环境：`tiup cluster check ./topology.yaml --apply`，自动修复系统参数（如关闭 swap）；

4. 部署集群：`tiup cluster deploy tidb-cluster v7.5.0 ./topology.yaml --user root`，指定集群名、版本和执行用户；

5. 启动集群：`tiup cluster start tidb-cluster`，启动后用 `tiup cluster display tidb-cluster` 确认所有节点状态为 Up。

扩容 1 个 TiKV 节点步骤：① 新建 `scale-out.yaml`，只写新增 TiKV 节点的拓扑信息；② 执行 `tiup cluster scale-out tidb-cluster ./scale-out.yaml`；③ 扩容完成后，用 `tiup cluster monitor tidb-cluster` 查看 PD 调度状态，约 10 分钟后 Region 会自动均衡到新节点。

**加分拓展**：部署时会通过 `--enable-monitor` 参数一并部署 Prometheus + Grafana 监控，默认 Grafana 端口 3000，方便后续查看集群指标。

## 场景6：TiKV 节点宕机，如何处理？

**面试官提问**：生产环境中 1 个 TiKV 节点突然宕机，你会怎么判断故障影响？如何恢复？如果节点彻底损坏，该怎么处理？

**标准回答**：分“故障判断-临时恢复-彻底损坏处理”三步：

1. 故障判断：① 用 `tiup cluster display tidb-cluster` 确认节点状态为 Down；② 登录 PD Dashboard，查看“Region 健康度”，确认该节点上的 Region 是否已触发 Leader 重选（正常 10 秒内完成）；③ 查看业务监控，确认是否有查询报错或延迟飙升（3 副本集群下，单节点宕机不影响服务）。

2. 临时恢复：① 登录宕机节点，查看 TiKV 日志（`tail -f /data/tikv/log/tikv.log`），若为进程挂掉，执行 `tiup cluster restart tidb-cluster -N 192.168.1.104:20160` 重启节点；② 若为磁盘临时故障，修复磁盘后重启，节点会自动同步缺失的数据（Raft 日志回放）。

3. 彻底损坏（如磁盘报废）：① 先移除故障节点：`tiup cluster scale-in tidb-cluster -N 192.168.1.104:20160`；② 更换新节点后，按扩容流程新增 TiKV 节点；③ 检查 PD 调度，确保所有 Region 副本数恢复为 3 个。

**加分拓展**：我们配置了 TiDB 告警规则，当 TiKV 节点宕机或 Region 副本数不足时，会同时触发短信和企业微信告警，确保 5 分钟内响应故障。

# 四、架构岗专属场景（侧重选型与架构设计）

## 场景7：TiDB 与 CockroachDB 选型，你怎么决策？

**面试官提问**：我们公司要做全球化电商平台，北美、欧洲、东南亚都有机房，数据库在 TiDB 和 CockroachDB 之间选，你推荐哪个？理由是什么？

**标准回答**：我会优先推荐 CockroachDB，核心原因是其“全球化部署能力”更贴合场景，但需满足一个前提——应用层能适配 PostgreSQL 协议。

具体对比决策点：① 跨地域事务一致性：CockroachDB 用混合逻辑时钟（HLC）实现无中心授时，跨洲部署时事务延迟比 TiDB 低 30%+，而 TiDB 依赖 PD Leader 发 TSO，跨地域时 TSO 延迟会导致事务性能下降；② 架构复杂度：CockroachDB 是对称架构，所有节点角色一致，跨地域部署时无需单独部署 PD 集群，运维更简单，TiDB 需在每个地域部署 PD 副本，架构更重；③ 数据同步：CockroachDB 原生支持“多活写入”，不同地域可同时写入，TiDB 目前更适合“一主多备”的异地灾备，多活写入需额外开发。

但如果公司应用是基于 MySQL 开发的，改造成本过高，就退选 TiDB，通过 Placement Rules 配置副本分布在不同地域，牺牲部分跨地域性能换应用兼容性。

**加分拓展**：之前调研过百度用 CockroachDB 的案例，他们通过修改 CockroachDB 源码适配了 MySQL 协议，既保留了全球化能力，又降低了应用改造开销，我们也可以评估这种方案。

## 场景8：TiDB 如何支撑 HTAP 混合负载？

**面试官提问**：我们需要一套数据库同时支撑电商订单交易（OLTP）和实时销量分析（OLAP），TiDB 怎么设计架构？如何避免分析查询影响交易性能？

**标准回答**：基于 TiDB + TiFlash 构建 HTAP 架构，核心是“读写分离 + 计算隔离”，具体设计分 4 层：

1. 接入层：用 Nginx 做负载均衡，将交易类请求（如创建订单、支付）路由到 TiDB Server 交易集群，分析类请求（如销量报表）路由到 TiDB Server 分析集群，避免请求排队；

2. 计算层：部署 2 组 TiDB Server，交易集群配置更高的并发连接数（如 1000），分析集群开启并行查询（`set tidb_enable_parallel_execution = 1`），提升复杂 SQL 执行效率；

3. 存储层：TiKV 集群只存储行数据，承接 OLTP 读写；给订单表、商品表创建 TiFlash 副本（`ALTER TABLE order_1t SET TIFLASH REPLICA 2`），TiFlash 按列存格式存储，分析查询时直接扫描列存，比 TiKV 快 5-10 倍；

4. 优化层：① 用 SQL Hint `/*+ READ_FROM_STORAGE(TIFLASH) */` 强制分析查询走 TiFlash；② 给分析场景的大表加分区（如按 create_time 月分区），减少扫描数据量；③ 限制分析查询的并发数（不超过 20），避免占用过多计算资源。

这套架构能保证交易查询延迟稳定在 50ms 内，分析查询延迟从原来的 10s 降至 500ms 内，且两者互不影响。

**加分拓展**：我们会用 TiDB Dashboard 的“负载均衡”功能，将 TiFlash 节点部署在独立的服务器上，与 TiKV 节点物理隔离，彻底避免 IO 资源竞争。
> （注：文档部分内容可能由 AI 生成）