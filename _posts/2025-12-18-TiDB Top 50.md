---
layout: post
title: TiDB 高频面试题 Top 50 参考答案（核心得分点版）
author: think-coding
categories: Interview
tags: [TiDB, 面试题, 核心得分点, 参考答案]
---

# TiDB 高频面试题 Top 50 参考答案（核心得分点版）
## 一、基础概念类（高频，30%）
1.  **什么是 TiDB？它属于哪种数据库类型？核心特性是什么？**
    答：TiDB 是 PingCAP 开源的**分布式 NewSQL 数据库**，融合关系型数据库的 ACID 特性与 NoSQL 的水平扩展能力。核心特性：① 高度兼容 MySQL 协议；② 水平扩展（计算/存储分离）；③ 强一致性多副本（Raft 协议）；④ 实时 HTAP（OLTP+OLAP 混合负载）；⑤ 高可用（故障自动恢复）。
    **得分点**：NewSQL 定位 + 5 大核心特性。

2.  **TiDB 解决了传统 MySQL 分库分表的哪些痛点？**
    答：解决 4 大痛点：① 分库分表需人工规划分片键，TiDB 自动分片；② 跨分片查询/事务复杂（需 2PC 或中间件），TiDB 原生支持分布式事务；③ 扩容需手动迁移数据，TiDB 无感知水平扩容；④ 运维成本高（需维护中间件+多套 MySQL），TiDB 统一集群管理。
    **得分点**：自动分片、分布式事务、无感知扩容、降低运维成本。

3.  **TiDB 的三层架构分别是什么？各层的核心作用是什么？**
    答：三层架构（计算与存储分离）：
    - **TiDB Server**：无状态 SQL 层，负责 SQL 解析、优化、执行，兼容 MySQL 协议，可水平扩展；
    - **PD（Placement Driver）**：集群大脑，管理元数据（Region 分布）、选主、负载均衡、副本调度；
    - **TiKV Server**：分布式存储层，基于 Raft 协议存储数据，按 Region 分片，提供强一致性多副本。
    **得分点**：三层名称 + 各层核心作用（无状态、元数据管理、Raft 存储）。

4.  **TiDB 为什么能做到 MySQL 协议兼容？应用迁移时需要注意什么？**
    答：兼容原理：TiDB Server 实现了 MySQL 网络协议、SQL 语法解析器和函数库。迁移注意事项：① 避免使用 MySQL 特有功能（如触发器、存储过程部分兼容）；② 自增主键在分布式场景下的全局唯一性（TiDB 用批量分配保证）；③ 索引类型限制（如不支持全文索引旧版本）；④ 事务隔离级别（TiDB 默认 RR，与 MySQL 一致）。
    **得分点**：协议/语法兼容 + 4 个迁移注意事项。

5.  **TiDB 是 OLTP、OLAP 还是 HTAP 数据库？为什么？**
    答：**HTAP 数据库**。原因：① TiKV 存储行数据，承接高并发 OLTP 事务（如订单交易）；② 搭配 TiFlash 列存引擎，实时同步 TiKV 数据，承接 OLAP 复杂分析（如报表统计）；③ 无需 ETL 同步，一份数据同时支撑两种负载，分析不影响交易性能。
    **得分点**：HTAP 定位 + TiKV/TiFlash 分工。

6.  **对比 TiDB 和传统 MySQL，核心差异有哪些？**
    答：
    | 维度         | 传统 MySQL | TiDB |
    |--------------|------------|------|
    | 架构         | 单体架构   | 分布式计算存储分离 |
    | 扩展能力     | 垂直扩展/分库分表 | 水平扩展（无感知） |
    | 一致性       | 单机 ACID  | 分布式强一致性（Raft） |
    | 负载支持     | 仅 OLTP    | HTAP（OLTP+OLAP） |
    **得分点**：架构、扩展、一致性、负载 4 个维度对比。

7.  **对比 TiDB 和 HBase，适用场景有何不同？**
    答：TiDB 是**结构化数据 + 强事务**场景（如订单、用户中心），兼容 SQL，支持 ACID；HBase 是**半结构化/非结构化 + 高吞吐写入**场景（如日志、时序数据），基于 KV 模型，事务能力弱（仅单行事务）。TiDB 适合需要复杂 SQL 查询和事务的业务，HBase 适合海量数据的写入和简单查询。
    **得分点**：结构化 vs 半结构化、强事务 vs 弱事务、SQL 支持 vs KV 模型。

8.  **TiDB 的 ACID 事务是如何保证的？支持哪些事务隔离级别？**
    答：ACID 保证：基于 **Percolator 模型 + 2PC 协议**，通过 Primary/Secondary 锁机制实现分布式事务；多副本通过 Raft 保证持久性（D）和一致性（C）。隔离级别：支持 **Read Committed（RC）、Repeatable Read（RR，默认）、Serializable**，RR 级别通过 MVCC 实现，避免幻读。
    **得分点**：Percolator+2PC + 隔离级别（默认 RR）。

9.  **什么是 TiKV？它的存储模型是什么？和 RocksDB 是什么关系？**
    答：TiKV 是 TiDB 的分布式存储引擎，核心存储模型是 **Key-Value 键值对**（底层将表数据按 RowKey 编码为 KV 存储）。TiKV 基于 **RocksDB** 作为单机存储引擎（RocksDB 负责本地磁盘的 KV 读写、压缩、持久化），TiKV 在此基础上增加了 Raft 多副本、分布式事务等能力。
    **得分点**：分布式存储引擎、KV 模型、基于 RocksDB 做分布式扩展。

10. **PD 的核心作用是什么？为什么 PD 集群需要部署奇数个节点？**
    答：PD 核心作用：① 管理集群元数据（Region 与 TiKV 节点映射）；② Raft 选主（PD Leader 决定 Region Leader）；③ 负载均衡（Region 调度）；④ 提供 TSO（时间戳）保证事务顺序。部署奇数节点：因为 Raft 协议需要**超过半数节点存活**才能选举 Leader，3 节点集群最多容忍 1 个节点故障，5 节点容忍 2 个，奇数节点容错率更高。
    **得分点**：4 大作用 + Raft 过半选举机制。

11. **TiFlash 是什么？它和 TiKV 的区别是什么？如何实现 HTAP 能力？**
    答：TiFlash 是 TiDB 的**列存引擎**，与 TiKV 实时同步数据。区别：TiKV 是行存，适合 OLTP 随机读写；TiFlash 是列存，适合 OLAP 批量分析（聚合、排序快）。HTAP 实现：TiDB 优化器自动将 OLAP 查询路由到 TiFlash，OLTP 查询路由到 TiKV，数据实时同步（延迟毫秒级），无需 ETL。
    **得分点**：列存引擎、行存 vs 列存、查询自动路由。

12. **TiDB 中的 Region 是什么？默认大小是多少？Region 分裂的触发条件是什么？**
    答：Region 是 TiKV 数据分片的最小单位，类似 MySQL 的分区，每个 Region 存储一段连续的 Key 范围数据。默认大小 **96MB**（可配置，建议 256MB 用于大表）。分裂触发条件：当 Region 大小超过阈值时，自动分裂为两个相等的子 Region，PD 负责调度新 Region 的副本。
    **得分点**：最小数据分片单位、默认 96MB、大小超过阈值分裂。

13. **TiDB 的分片键和传统分库分表的分片键有什么区别？如何选择合适的分片键？**
    答：区别：① TiDB 分片键 = 主键，由数据库自动分片，无需手动维护物理分表；② 传统分库分表分片键是逻辑规则，需手动创建物理分表。分片键选择原则：① 高频查询字段作为前缀（如 user_id）；② 分布均匀（避免热点，如自增 ID、用户 ID）；③ 减少跨 Region 事务（同事务数据尽量在一个 Region）。
    **得分点**：自动 vs 手动分片 + 3 个选择原则。

14. **TiDB 支持哪些索引类型？全局索引和普通索引的区别是什么？**
    答：支持索引类型：**主键索引（聚簇索引）、二级索引、唯一索引**，不支持全文索引（新版本可通过插件实现）。全局索引 vs 普通索引：TiDB 中所有索引都是**全局索引**（索引数据分布在所有 TiKV 节点），区别于传统分库分表的局部索引（仅在分片内有效），TiDB 全局索引无需跨分片查询，性能更高。
    **得分点**：3 种索引类型 + 全局索引覆盖所有分片。

15. **什么是 Raft 协议？TiDB 中哪些组件用到了 Raft 协议？**
    答：Raft 是**分布式一致性算法**，通过“选举 Leader + 日志复制”保证多副本数据一致，分为 Leader/Follower/Candidate 三个角色。TiDB 中 **PD 集群**和 **TiKV 集群**都用 Raft 协议：PD 用 Raft 选举 PD Leader，管理元数据；TiKV 用 Raft 管理 Region 多副本，保证数据强一致性。
    **得分点**：一致性算法 + PD/TiKV 两个组件。

## 二、架构原理类（极高频，35%）
16. **TiDB Server 是无状态的，具体体现在哪里？这种设计的优势是什么？**
    答：无状态体现：TiDB Server 不存储任何数据，所有数据都在 TiKV 中；SQL 执行的上下文信息不持久化，重启后不影响集群。优势：① 水平扩展简单（新增 TiDB 节点即可提升并发能力）；② 无单点故障（前端 LB 轮询分发请求，单个节点故障不影响服务）；③ 运维成本低（节点可随意启停）。
    **得分点**：不存数据/无持久化上下文 + 水平扩展/无单点故障。

17. **PD 如何管理 TiKV 的 Region？负载均衡的核心策略是什么？**
    答：PD 管理 Region：① 维护 Region 元数据（每个 Region 的 Leader/Follower 分布、Key 范围）；② 监控 Region 状态（大小、读写热点）；③ 触发 Region 分裂/合并；④ 调度 Region 副本到不同节点。负载均衡策略：① 节点间 Region 数量均衡；② Leader 节点均衡（避免单个 TiKV 节点承担过多 Leader 读写）；③ 副本按 Label 分布（如跨 AZ 部署）。
    **得分点**：4 项管理动作 + 3 个均衡策略。

18. **TiKV 的多副本机制是怎样的？默认副本数是多少？如何保证数据强一致性？**
    答：多副本机制：每个 Region 默认 **3 副本**，分为 1 个 Leader 和 2 个 Follower，副本分散在不同 TiKV 节点（或 AZ）。强一致性保证：① 写请求只能由 Leader 处理；② Leader 将写操作记录为日志，同步到超过半数 Follower；③ 半数 Follower 确认后，Leader 提交日志并返回客户端；④ Follower 同步日志后，数据与 Leader 一致。
    **得分点**：3 副本、Leader 写、过半确认提交。

19. **TiDB 的分布式事务实现原理是什么？（Percolator 模型/2PC 流程）**
    答：基于 **Percolator 模型 + 两阶段提交（2PC）**，核心是 Primary/Secondary 锁机制：
    1.  **第一阶段（准备阶段）**：事务选择一个 Primary Row（主键行），给所有涉及的行加锁，记录锁信息到 TiKV；
    2.  **第二阶段（提交阶段）**：① 若所有行加锁成功，先提交 Primary Row，再异步提交其他 Secondary Row；② 若加锁失败，回滚所有行。
    同时 PD 提供 TSO 时间戳，保证事务的全局顺序。
    **得分点**：Percolator 模型、2PC 两阶段、Primary 锁、TSO 时间戳。

20. **TiDB 中 Leader 节点的作用是什么？TiKV Region Leader 和 PD Leader 的区别是什么？**
    答：Leader 作用：负责处理读写请求，Follower 仅同步数据，不处理写请求。区别：① **PD Leader**：整个集群的唯一 Leader，管理元数据、选主、调度，PD Follower 仅参与投票；② **TiKV Region Leader**：每个 Region 独立选举 Leader，负责该 Region 的读写，不同 Region 的 Leader 可分布在不同 TiKV 节点。
    **得分点**：Leader 处理读写 + PD Leader 集群唯一、Region Leader 分片独立。

21. **TiDB 如何处理读写请求？读请求的流程是什么？写请求的流程是什么？**
    答：**读请求流程**：客户端 → TiDB Server → PD 查询 Region 元数据 → TiKV Region Leader 读取数据 → 返回结果。
    **写请求流程**：客户端 → TiDB Server → PD 查询 Region 元数据 → TiKV Region Leader 接收写请求 → 同步日志到 Follower → 过半 Follower 确认 → 提交日志 → 返回客户端。
    **得分点**：读请求查 PD 找 Region Leader；写请求需 Raft 过半确认。

22. **TiFlash 如何和 TiKV 保持数据同步？同步延迟是多少？**
    答：同步原理：TiKV 的 Region Leader 会将数据变更日志同步给 TiFlash 副本（TiFlash 作为 Raft Learner 角色，只同步日志不参与选举），TiFlash 按列存格式组织数据。同步延迟：**毫秒级**，满足实时 OLAP 分析需求，可通过监控查看延迟指标。
    **得分点**：Raft Learner 同步日志、毫秒级延迟。

23. **TiDB 中的 MVCC 机制是如何实现的？有什么作用？**
    答：MVCC（多版本并发控制）实现：TiDB 给每个事务分配唯一 TSO 时间戳，每行数据存储多个版本，每个版本关联一个时间戳。读取时，只读取时间戳小于当前事务 TSO 的最新版本。作用：① 避免读写阻塞（读不锁写，写不锁读）；② 实现事务隔离级别（如 RR）；③ 支持历史数据查询。
    **得分点**：TSO 时间戳 + 多版本数据 + 读写不阻塞。

24. **什么是 Placement Rules？如何通过它实现多可用区部署？**
    答：Placement Rules 是 PD 提供的**副本调度规则**，可通过 Label（如 az、host）定义副本的分布策略。多 AZ 部署实现：① 给 TiKV 节点打 Label（如 `az=az1`、`az=az2`、`az=az3`）；② 配置 Placement Rules，要求每个 Region 的 3 副本分别分布在 3 个不同 AZ；③ PD 自动调度副本，保证单个 AZ 故障时，其他 AZ 有可用副本。
    **得分点**：副本调度规则、Label 标记节点、跨 AZ 副本分布。

25. **TiDB 支持哪些数据导入/导出工具？TiDB Lightning 的物理导入和逻辑导入有什么区别？**
    答：导入工具：TiDB Lightning（批量导入）、LOAD DATA（小数据量）；导出工具：Dumpling。物理导入 vs 逻辑导入：
    | 维度         | 物理导入 | 逻辑导入 |
    |--------------|----------|----------|
    | 原理         | 直接写入 TiKV 底层数据格式 | 通过 SQL 执行 INSERT 语句 |
    | 速度         | 极快（GB/分钟级） | 较慢（MB/分钟级） |
    | 适用场景     | 海量数据（如 1TB 大表） | 小数据量 |
    | 锁机制       | 需锁表 | 无需锁表 |
    **得分点**：工具名称 + 速度/原理/场景对比。

26. **TiDB CDC 的作用是什么？如何实现增量数据同步？**
    答：TiDB CDC（Change Data Capture）是**增量数据同步工具**，用于捕获 TiDB 的数据变更（INSERT/UPDATE/DELETE），同步到下游系统（如 MySQL、Kafka）。实现原理：① 订阅 TiKV 的 Region 变更日志；② 按事务顺序解析日志；③ 将变更数据输出到下游；④ 保证数据的一致性和顺序性。
    **得分点**：增量数据同步、订阅 TiKV 日志、事务顺序输出。

27. **TiDB 的索引下推、分区裁剪等优化手段是如何提升查询性能的？**
    答：① **索引下推**：将 WHERE 条件中的过滤操作下推到 TiKV 节点执行，减少 TiKV 返回给 TiDB Server 的数据量，降低网络传输开销；② **分区裁剪**：查询时只扫描与条件匹配的 Region（或分区），避免全表扫描，尤其适合大表的范围查询。两者都能减少无效数据的扫描和传输，提升查询速度。
    **得分点**：索引下推减少数据传输、分区裁剪减少扫描范围。

28. **TiDB 中的悲观锁和乐观锁有什么区别？默认使用哪种？适用场景是什么？**
    答：区别：
    | 维度         | 悲观锁 | 乐观锁 |
    |--------------|--------|--------|
    | 加锁时机     | 事务开始时加锁 | 事务提交时检查冲突 |
    | 冲突处理     | 阻塞等待锁释放 | 冲突时回滚重试 |
    | 适用场景     | 高冲突场景（如秒杀） | 低冲突场景（如普通订单） |
    TiDB **默认使用悲观锁**，与 MySQL 行为一致；乐观锁可通过 `SET tidb_txn_mode = 'optimistic'` 开启。
    **得分点**：加锁时机/冲突处理/场景 + 默认悲观锁。

29. **为什么 TiDB 不建议使用自增主键作为分片键（在某些场景下）？**
    答：原因：自增主键是单调递增的，写入时所有新数据都会集中到**最后一个 Region**，导致该 Region 成为写入热点（所有写请求都打在一个 TiKV 节点），无法发挥分布式集群的性能优势。适用场景：若业务查询以主键为主，可使用；若有大量用户维度查询，建议用 `user_id + id` 复合主键作为分片键。
    **得分点**：写入热点、集中在最后一个 Region。

30. **TiDB 如何处理大事务？大事务会带来哪些问题？**
    答：大事务问题：① 占用大量内存和磁盘空间；② 锁持有时间长，阻塞其他事务；③ 增加 Raft 同步开销；④ 容易触发 `txn-too-large` 报错。处理方案：① 拆分大事务为多个小事务；② 批量操作时加 LIMIT（如每次更新 1000 条）；③ 避免一次性扫描大量数据；④ 调大事务相关参数（如 `tidb_txn_total_size_limit`），但不建议过度调大。
    **得分点**：4 个问题 + 拆分小事务/批量 LIMIT。

## 三、运维调优类（高频，25%）
31. **TiDB 集群的最小部署架构是什么？生产环境建议如何部署？**
    答：最小部署架构（测试环境）：**1 个 TiDB Server + 1 个 PD Server + 1 个 TiKV Server**（无高可用，仅用于测试）。生产环境建议：① 3 个 PD Server（奇数节点，高可用）；② 3+ 个 TiDB Server（根据并发需求扩展）；③ 3+ 个 TiKV Server（3 副本，分散在不同 AZ/物理机）；④ 可选 2+ 个 TiFlash Server（用于 HTAP 分析）。
    **得分点**：最小 1+1+1、生产 3PD+3TiKV+多 TiDB。

32. **TiUP 是什么？它的核心功能有哪些？如何用 TiUP 部署 TiDB 集群？**
    答：TiUP 是 TiDB 官方的**集群运维工具**，用于部署、管理、升级 TiDB 集群。核心功能：① 集群部署（支持物理机/K8s）；② 集群启停、扩缩容；③ 版本升级；④ 配置管理；⑤ 监控部署。部署步骤：① 安装 TiUP；② 生成拓扑文件（定义节点角色和 IP）；③ 执行 `tiup cluster deploy` 部署集群；④ 执行 `tiup cluster start` 启动集群。
    **得分点**：运维工具 + 5 大功能 + 4 步部署流程。

33. **TiDB 常见的监控指标有哪些？如何通过监控定位性能问题？**
    答：核心监控指标：① **TiDB 指标**：QPS/TPS、延迟（95th/99th）、慢查询数、连接数；② **TiKV 指标**：Region 数量、Leader 数量、Raft 同步延迟、磁盘 IOPS/吞吐量；③ **PD 指标**：PD Leader 存活状态、TSO 延迟、Region 调度次数。定位性能问题：① 看 TiDB 慢查询，分析 SQL 执行计划；② 看 TiKV 磁盘 IOPS 是否瓶颈；③ 看 Leader 分布是否均衡（避免热点）；④ 看 Raft 同步延迟是否过高。
    **得分点**：3 类指标 + 慢查询/IO/Leader 分布/同步延迟 4 个定位方向。

34. **TiDB 集群扩容的步骤是什么？扩容后 PD 会做哪些操作？**
    答：扩容步骤（以 TiKV 为例）：① 修改拓扑文件，新增 TiKV 节点；② 执行 `tiup cluster scale-out` 扩容；③ 等待节点加入集群。PD 操作：① 自动将新增节点标记为可用；② 启动 Region 调度，将部分 Region 副本迁移到新节点；③ 平衡各节点的 Region 数量和 Leader 数量，实现负载均衡。
    **得分点**：3 步扩容 + PD 调度 Region 负载均衡。

35. **TiDB 中的热点问题有哪些类型？如何发现和解决热点？**
    答：热点类型：① **写入热点**（如自增主键导致最后一个 Region 写入集中）；② **读取热点**（如高频查询某个用户的订单）。发现方法：① 通过 TiDB Dashboard 的热点分析页面；② 监控 TiKV 节点的读写吞吐量（某个节点远高于其他节点）。解决方法：① 优化分片键（如复合键 `user_id + id`）；② 打散热点数据（如哈希分片）；③ 读写分离（读请求路由到 Follower）；④ 限制单请求 QPS。
    **得分点**：2 种热点类型 + 监控发现 + 4 种解决方法。

36. **TiDB 如何做备份与恢复？全量备份和增量备份的方案是什么？**
    答：备份工具：**BR（Backup & Restore）**（分布式备份，适合大规模集群）、Dumpling（逻辑备份，适合小数据量）。全量备份：① 用 BR 备份整个集群数据到外部存储（如 S3、HDFS）；② 支持快照备份，不影响业务。增量备份：① 基于 TiDB CDC 捕获增量数据，同步到下游存储；② 全量备份 + CDC 增量日志，实现任意时间点恢复（PITR）。
    **得分点**：BR/Dumpling 工具 + 全量快照备份 + 增量 CDC 同步。

37. **TiDB 性能调优的核心方向有哪些？（分片键、索引、参数、硬件）**
    答：4 大核心方向：① **分片键优化**：选择分布均匀、高频查询的字段作为前缀；② **索引优化**：减少冗余索引，增加联合索引，利用索引下推；③ **参数调优**：调大 TiKV RocksDB 写缓冲区、调整 Raft 线程数、开启并行查询；④ **硬件优化**：TiKV 用 SSD 磁盘、万兆网卡，PD 用高性能 SSD（存储元数据）。
    **得分点**：分片键/索引/参数/硬件 4 个方向。

38. **TiKV 的 RocksDB 有哪些关键参数可以调优？（如 write-buffer-size、compaction 策略）**
    答：关键参数：① `write-buffer-size`：RocksDB 内存写缓冲区大小，调大（如 256MB）可减少刷盘次数，提升写入性能；② `max-bytes-for-level-base`：Level 0 层最大数据量，调大减少 compaction 频率；③ `compaction-style`：压缩策略，选择 `level`（适合读多写少）或 `universal`（适合写多读少）；④ `block-cache-size`：数据块缓存大小，调大提升读性能。
    **得分点**：write-buffer-size、compaction 策略、block-cache-size。

39. **TiDB 中如何解决数据倾斜问题？**
    答：数据倾斜原因：分片键分布不均（如超级用户订单量过大）。解决方法：① **优化分片键**：用复合键（如 `user_id + create_time`）打散数据；② **哈希分片**：对分片键进行哈希取模（如 `user_id % 1024`），分散超级用户数据；③ **手动拆分热点 Region**：通过 PD 命令将热点 Region 拆分为更小的 Region；④ **业务拆分**：将超级用户的数据单独存储。
    **得分点**：复合键/哈希分片/手动拆分/业务拆分。

40. **TiDB 集群出现节点宕机后，自动恢复的流程是什么？**
    答：以 TiKV 节点宕机为例：① PD 检测到 TiKV 节点失联（心跳超时）；② 该节点上的所有 Region Leader 触发重新选举（Follower 升级为 Leader）；③ PD 调度这些 Region 的副本到其他存活节点，补充副本数量到 3 个；④ 节点恢复后，PD 将其作为 Follower 重新加入集群，同步数据。整个过程自动完成，无需人工干预。
    **得分点**：PD 检测失联 → Leader 重选 → 副本补充 → 节点恢复同步。

41. **TiDB 支持哪些高可用架构？同城多 AZ 和异地灾备的部署方案是什么？**
    答：支持高可用架构：单机房高可用、同城多 AZ、异地灾备、异地多活。① **同城多 AZ**：3 个 AZ 部署，每个 Region 的 3 副本分布在 3 个 AZ，单个 AZ 故障不影响服务；② **异地灾备**：主集群（同城 3 AZ）+ 异地灾备集群，通过 TiDB CDC 同步数据，主集群故障时，手动切换到灾备集群；或主集群部署异地 Learner 副本，故障时升级为投票副本。
    **得分点**：4 种架构 + 同城多 AZ 副本分布 + 异地 CDC 同步。

42. **TiDB 和 Redis 二级缓存结合时，需要注意什么问题？**
    答：注意 3 大核心问题：① **缓存一致性**：更新数据时，先更数据库，再删 Redis 缓存，最后失效本地 Caffeine 缓存；避免直接更新缓存；② **过期时间**：Caffeine 过期时间短于 Redis，减少本地脏数据；③ **缓存穿透/击穿/雪崩**：Redis 缓存空值防穿透，热点 key 本地缓存防击穿，过期时间随机打散防雪崩。
    **得分点**：缓存一致性、过期时间、三大缓存问题。

43. **TiDB 中的慢查询如何优化？常用的优化手段有哪些？**
    答：优化步骤：① 查看慢查询日志或 TiDB Dashboard，获取 SQL 和执行计划；② 分析是否走索引（避免全表扫描）；③ 优化索引（增加联合索引、删除冗余索引）；④ 优化 SQL（避免 SELECT *、减少 JOIN 表数、用 LIMIT 限制结果集）；⑤ 开启索引下推、并行查询等优化开关；⑥ 对于 OLAP 查询，路由到 TiFlash 执行。
    **得分点**：查看执行计划 + 索引优化 + SQL 优化 + TiFlash 路由。

44. **生产环境中，TiDB 集群的硬件选型有什么建议？（CPU、内存、磁盘）**
    答：硬件选型建议：
    | 组件       | CPU | 内存 | 磁盘 |
    |------------|-----|------|------|
    | TiDB Server | 16C/32C | 32G/64G | SATA SSD（无数据存储，仅需高速磁盘） |
    | PD Server | 8C/16C | 16G/32G | NVMe SSD（元数据读写频繁，需低延迟） |
    | TiKV Server | 24C/48C | 64G/128G | NVMe SSD（核心存储，IO 密集型） |
    网络：万兆网卡，减少节点间通信延迟。
    **得分点**：TiKV 用 NVMe SSD、PD 用 NVMe SSD、万兆网卡。

45. **TiDB 如何做版本升级？升级过程中如何保证业务不中断？**
    答：升级方式：**滚动升级**（通过 TiUP 实现），保证业务不中断。步骤：① 升级 TiUP 工具；② 升级 PD 集群（先升级 Follower，再升级 Leader，逐个节点升级，升级后重启）；③ 升级 TiKV 集群（逐个节点升级，Leader 自动迁移，无服务中断）；④ 升级 TiDB Server（逐个节点升级，前端 LB 摘除节点，升级后重新接入）；⑤ 升级 TiFlash（可选）。核心：滚动升级，逐个节点操作，避免集群整体不可用。
    **得分点**：滚动升级 + PD/TiKV/TiDB 升级顺序 + LB 摘除节点。

## 四、综合对比与选型类（中频，10%）
46. **TiDB 和 CockroachDB 的核心差异是什么？选型时如何决策？**
    答：核心差异：
    | 维度         | TiDB | CockroachDB |
    |--------------|------|-------------|
    | 协议兼容     | MySQL | PostgreSQL |
    | 架构         | 三层架构（TiDB/PD/TiKV） | 对称架构（无独立 PD） |
    | 负载支持     | HTAP（TiKV+TiFlash） | 仅 OLTP |
    | 开源协议     | Apache 2.0（完全开源） | AGPL 3.0（开源版功能受限） |
    选型决策：① 从 MySQL 迁移 → 选 TiDB；② 从 PostgreSQL 迁移 → 选 CockroachDB；③ 需要 HTAP 混合负载 → 选 TiDB；④ 全球化部署、极简运维 → 选 CockroachDB。
    **得分点**：协议/架构/负载/协议 4 个差异 + 选型场景。

47. **TiDB 和 MySQL 分库分表（如 Sharding-JDBC）相比，优势和劣势是什么？**
    答：优势：① 自动分片，无需人工维护物理分表；② 原生支持分布式事务，跨分片事务简单；③ 水平扩展无感知，扩容无需迁移数据；④ 支持 HTAP 混合负载；⑤ 运维成本低。劣势：① 部署复杂度高于 MySQL 分库分表；② 对硬件要求更高（需 SSD）；③ 部分 MySQL 特有功能兼容不足。
    **得分点**：自动分片/分布式事务/无感知扩容 + 部署复杂/硬件要求高。

48. **TiDB 适合哪些业务场景？不适合哪些场景？**
    答：**适合场景**：① 替代 MySQL 分库分表的核心 OLTP 场景（如订单、用户中心）；② 需实时分析的 HTAP 场景（如电商实时报表）；③ 数据量大（TB/PB 级）、高并发读写场景；④ 需高可用、强一致性的金融级场景。
    **不适合场景**：① 小规模数据、低并发场景（用 MySQL 更轻量）；② 纯 OLAP 场景（用 ClickHouse 更优）；③ 对延迟极度敏感的场景（如高频交易，需内存数据库）。
    **得分点**：4 个适合场景 + 3 个不适合场景。

49. **TiDB 社区版和企业版的核心区别是什么？如何选择？**
    答：核心区别：
    | 维度         | 社区版 | 企业版 |
    |--------------|--------|--------|
    | 核心功能     | 开源核心能力（分布式事务、水平扩展） | 社区版功能 + 商业增强特性 |
    | 安全合规     | 基础安全 | 信创合规、审计、访问白名单 |
    | 运维工具     | 命令行工具 | 图形化运维平台 |
    | 技术支持     | 社区支持 | 官方原厂支持 |
    选型决策：① 个人/中小团队、非核心业务 → 选社区版；② 企业核心业务、需合规和技术支持 → 选企业版；③ 金融/政务等行业 → 选企业版。
    **得分点**：安全合规/运维工具/技术支持 + 选型场景。

50. **国产化数据库选型中，TiDB 的核心竞争力是什么？和其他国产数据库（如人大金仓、达梦）相比有何优势？**
    答：核心竞争力：① **分布式架构**：天然支持水平扩展，解决传统国产数据库（人大金仓、达梦）单体架构的扩展瓶颈；② **MySQL 兼容**：迁移成本低，适合存量 MySQL 业务国产化替代；③ **HTAP 能力**：一份数据支撑 OLTP+OLAP，无需多套数据库；④ **开源生态**：社区活跃，文档丰富，避免厂商绑定。相比传统国产数据库，TiDB 更适合大规模、高并发的分布式业务场景。
    **得分点**：分布式架构/MySQL 兼容/HTAP/开源生态 + 适合大规模分布式场景。

---